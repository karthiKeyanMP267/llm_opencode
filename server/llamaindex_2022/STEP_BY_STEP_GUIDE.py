"""
Step-by-Step Execution Guide
Run these commands in sequence to ingest your PDFs
"""

# ============================================================================
# STEP 1: Activate Conda Environment
# ============================================================================
# conda activate gpu-env # type: ignore

# Expected Output:
# (gpu-env) C:\Users\vikym\Documents\GitHub\llmAgent>


# ============================================================================
# STEP 2: Install Dependencies (First Time Only)
# ============================================================================
# pip install -r llamaindex_requirements.txt

# Expected Output:
# Collecting llama-index>=0.10.0
# Collecting llama-index-core>=0.10.0
# ...
# Successfully installed llama-index-0.10.12 chromadb-0.4.22 ...


# ============================================================================
# STEP 3: Run Quick Start Script (Automated)
# ============================================================================
# .\quick_start_llamaindex.ps1

# This runs all steps automatically:
# - Dependency check
# - PDF discovery
# - Ingestion pipeline
# - Test query

# OR run manually (see steps below)...


# ============================================================================
# STEP 4: Run Ingestion Pipeline (Manual Method)
# ============================================================================
# python llamaindex_pdf_ingestion.py --source-dir "D:\2022" --chroma-path "./chroma_data"

# Expected Output:
"""
================================================================================
ğŸ“ KEC PDF Ingestion Pipeline - LlamaIndex + ChromaDB
================================================================================

ğŸ“‚ Source Directory: D:\2022
ğŸ’¾ ChromaDB Path: ./chroma_data
ğŸ“Š Collection Name: kec_syllabi_regulations_r2022

âš™ï¸  Loading embedding model: BAAI/bge-small-en-v1.5
   - Chunk size: 512
   - Chunk overlap: 50

ğŸ—„ï¸  Initializing ChromaDB...
   âœ“ Created collection: kec_syllabi_regulations_r2022

ğŸ” Discovering PDF files...
   âœ“ Found 45 PDF files

ğŸ“š Processing 45 PDF files...
================================================================================
Processing PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [05:23<00:00,  7.19s/file]

ğŸ“„ Processing: KEC-R2022-CSE.pdf
   Path: Curricula and Syllabi\UG\BE\KEC-R2022-CSE.pdf
   âœ“ Extracted 156 pages/chunks

ğŸ“„ Processing: R2022-MBA.pdf
   Path: Curricula and Syllabi\PG\MBA\R2022-MBA.pdf
   âœ“ Extracted 98 pages/chunks

... (continues for all PDFs)

================================================================================
ğŸ“Š Extraction Complete:
   - Total PDFs: 45
   - Successful: 45
   - Failed: 0
   - Total document chunks: 2341

================================================================================
ğŸ”„ Creating vector index and storing in ChromaDB...
   (This may take a few minutes depending on document count)
Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2341/2341 [02:15<00:00, 17.31it/s]

âœ… Ingestion Complete!
   - Total chunks stored: 2341
   - ChromaDB collection: kec_syllabi_regulations_r2022
   - Storage path: ./chroma_data

ğŸ“Š Statistics saved to: chroma_data\kec_syllabi_regulations_r2022_stats.json

================================================================================
âœ… Pipeline Complete!
================================================================================

ğŸš€ Next Steps:
   1. Use your existing MCP server to query the collection
   2. Collection name: kec_syllabi_regulations_r2022
   3. ChromaDB path: ./chroma_data

ğŸ’¡ MCP Server Usage:
   Run: python chromaDB_MCP/mcp_chroma_server.py \
        --client-type persistent \
        --data-dir ./chroma_data
"""


# ============================================================================
# STEP 5: Test Queries (Verify Everything Works)
# ============================================================================

# Run predefined test queries
# python test_llamaindex_query.py --chroma-path ./chroma_data

# Expected Output:
"""
================================================================================
ğŸ” LlamaIndex Query Test - KEC Syllabi Database
================================================================================

âš™ï¸  Loading embedding model...
ğŸ“‚ Connecting to ChromaDB: ./chroma_data
âœ“ Collection found: kec_syllabi_regulations_r2022
  - Total chunks: 2341

ğŸ”„ Creating query engine...

================================================================================
ğŸ“ Running Test Queries...
================================================================================


================================================================================
Query 1: What are the prerequisite courses for Machine Learning?
================================================================================

ğŸ¤– Response:
--------------------------------------------------------------------------------
The prerequisite courses for Machine Learning include:
1. Data Structures and Algorithms (CS101)
2. Linear Algebra and Probability (MA201)
3. Python Programming (CS102)

ğŸ“š Source Documents:
--------------------------------------------------------------------------------

1. File: KEC-R2022-AIML.pdf
   Level: UG
   Program: BTECH
   Department: AIML
   Category: Curricula and Syllabi
   Score: 0.8234
   Preview: Machine Learning (CS301) Prerequisites: Students must have completed 
   Data Structures and Algorithms (CS101), Linear Algebra and Probability...

2. File: KEC-R2022-CSE.pdf
   Level: UG
   Program: BE
   Department: CSE
   Category: Curricula and Syllabi
   Score: 0.7892
   Preview: Course Code: CS301 - Machine Learning Prerequisites: CS101, MA201...
"""


# ============================================================================
# STEP 6: Run Custom Query
# ============================================================================
# python test_llamaindex_query.py --chroma-path ./chroma_data --query "What is the attendance policy?"


# ============================================================================
# STEP 7: Interactive Query Mode (Ask Multiple Questions)
# ============================================================================
# python test_llamaindex_query.py --chroma-path ./chroma_data --interactive

# Expected Output:
"""
================================================================================
ğŸ’¬ Interactive Query Mode
================================================================================
Type your questions (or 'quit' to exit)

--------------------------------------------------------------------------------

â“ Your question: What are the lab requirements for CSE?

ğŸ¤– Searching...

ğŸ“– Answer:
--------------------------------------------------------------------------------
The lab requirements for Computer Science Engineering include:
- Minimum 3 lab courses per semester
- Each lab carries 1.5 credits
- Mandatory attendance of 75%
- Lab record submission required

ğŸ“š Sources:
  1. KEC-R2022-CSE.pdf (CSE)
  2. R2022-BEBTech-Regulations.pdf (BE)

--------------------------------------------------------------------------------

â“ Your question: quit

ğŸ‘‹ Goodbye!
"""


# ============================================================================
# STEP 8: Start MCP Server (For Claude Desktop)
# ============================================================================
# python chromaDB_MCP/mcp_chroma_server.py --client-type persistent --data-dir ./chroma_data

# Expected Output:
"""
Starting Chroma MCP Server...
âœ“ Connected to ChromaDB at ./chroma_data
âœ“ Available collections: kec_syllabi_regulations_r2022
âœ“ Server ready for MCP connections
"""


# ============================================================================
# VERIFICATION CHECKLIST
# ============================================================================

# âœ… Check 1: Verify ChromaDB contains data
# python -c "import chromadb; client = chromadb.PersistentClient(path='./chroma_data'); collection = client.get_collection('kec_syllabi_regulations_r2022'); print(f'âœ“ Collection has {collection.count()} chunks')"

# Expected Output:
# âœ“ Collection has 2341 chunks


# âœ… Check 2: Verify metadata is present
# python -c "import chromadb; client = chromadb.PersistentClient(path='./chroma_data'); collection = client.get_collection('kec_syllabi_regulations_r2022'); result = collection.get(limit=1, include=['metadatas']); print('âœ“ Sample metadata:', result['metadatas'][0])"

# Expected Output:
# âœ“ Sample metadata: {'source_file': 'KEC-R2022-CSE.pdf', 'level': 'UG', 'program_type': 'BE', 'department': 'CSE', 'category': 'Curricula and Syllabi', 'regulation': 'R2022', ...}


# âœ… Check 3: View ingestion statistics
# python -c "import json; stats = json.load(open('./chroma_data/kec_syllabi_regulations_r2022_stats.json')); print(f\"âœ“ Processed {stats['successful']}/{stats['total_pdfs']} PDFs\"); print(f\"âœ“ Total chunks: {stats['total_chunks']}\")"

# Expected Output:
# âœ“ Processed 45/45 PDFs
# âœ“ Total chunks: 2341


# ============================================================================
# TROUBLESHOOTING COMMANDS
# ============================================================================

# If you get "collection not found" error:
# python -c "import chromadb; client = chromadb.PersistentClient(path='./chroma_data'); print('Available collections:', [c.name for c in client.list_collections()])"

# If embeddings are slow (check GPU availability):
# python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"

# If you need to delete and re-run:
# python -c "import chromadb; client = chromadb.PersistentClient(path='./chroma_data'); client.delete_collection('kec_syllabi_regulations_r2022'); print('âœ“ Collection deleted')"


# ============================================================================
# EXPECTED FILE STRUCTURE AFTER COMPLETION
# ============================================================================
"""
llmAgent/
â”œâ”€â”€ chroma_data/                                    # ChromaDB storage
â”‚   â”œâ”€â”€ chroma.sqlite3                             # Database file
â”‚   â””â”€â”€ kec_syllabi_regulations_r2022_stats.json   # Ingestion statistics
â”œâ”€â”€ llamaindex_pdf_ingestion.py                    # Main ingestion script âœ“
â”œâ”€â”€ test_llamaindex_query.py                       # Query test script âœ“
â”œâ”€â”€ llamaindex_requirements.txt                    # Dependencies âœ“
â”œâ”€â”€ quick_start_llamaindex.ps1                     # Automated setup âœ“
â”œâ”€â”€ LLAMAINDEX_PIPELINE_README.md                  # Full documentation âœ“
â””â”€â”€ STEP_BY_STEP_GUIDE.py                          # This file âœ“
"""


# ============================================================================
# TIME ESTIMATES
# ============================================================================
"""
Activity                    Time          Notes
---------------------------------------------------------------------------
First-time pip install      2-3 min       One-time only
PDF discovery               < 5 sec       Fast directory scan
Text extraction             3-5 min       45 PDFs, ~2000 pages
Embedding generation        2-4 min       CPU: slower, GPU: faster
ChromaDB storage            10-20 sec     Batch insertion
Test queries                5-10 sec      For 3-5 queries
---------------------------------------------------------------------------
TOTAL (first run)           8-12 min      For ~45 PDFs
Subsequent queries          < 5 sec       Lightning fast!
"""


# ============================================================================
# PERFORMANCE TIPS
# ============================================================================
"""
1. Use GPU if available (automatically detected):
   - Embeddings: 50-100 it/s (vs 10-15 on CPU)
   - Total time: 3-5 min (vs 8-12 min)

2. Adjust chunk size for better results:
   - Smaller chunks (256): More precise, more chunks
   - Larger chunks (1024): More context, fewer chunks

3. Try different embedding models:
   - BAAI/bge-small-en-v1.5: Fast, good quality (default)
   - BAAI/bge-base-en-v1.5: Slower, better quality
   - sentence-transformers/all-MiniLM-L6-v2: Very fast, decent quality

4. Batch processing for large datasets:
   - Process folders separately
   - Merge collections later
"""


# ============================================================================
# USAGE IN CLAUDE DESKTOP
# ============================================================================
"""
After ingestion, use these prompts in Claude Desktop:

1. "Search the KEC syllabi for Machine Learning prerequisites"
   â†’ Uses MCP tool: query_collection

2. "Show me all courses in the CSE department"
   â†’ Uses metadata filtering

3. "What's the attendance policy for UG students?"
   â†’ Semantic search with context

4. "Compare credit structures between BE and BTECH programs"
   â†’ Multi-document comparison

5. "List all lab courses in the first semester"
   â†’ Structured data extraction
"""


# ============================================================================
# END OF GUIDE
# ============================================================================
print("âœ… All commands documented!")
print("ğŸ“– See LLAMAINDEX_PIPELINE_README.md for detailed explanations")
